{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49759fa5",
   "metadata": {
    "id": "49759fa5"
   },
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d332a6a",
   "metadata": {
    "id": "8d332a6a"
   },
   "source": [
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a95a1b5",
   "metadata": {
    "executionInfo": {
     "elapsed": 6031,
     "status": "ok",
     "timestamp": 1718035494519,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "3a95a1b5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchaudio\n",
    "from os import path, walk\n",
    "import torch.nn as nn\n",
    "from IPython.display import Audio, display\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d311b65",
   "metadata": {
    "id": "1d311b65"
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf53008",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1718035494519,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "4cf53008",
    "outputId": "143c4a6a-122d-444a-8e0f-0370961123e8"
   },
   "outputs": [],
   "source": [
    "fs = 8000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f220b9f",
   "metadata": {
    "id": "2f220b9f"
   },
   "source": [
    "### Data collecting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168f7eb",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1718035494519,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "0168f7eb"
   },
   "outputs": [],
   "source": [
    "def initial_states_to_list(random_sampling_file):\n",
    "    initial_states = list()\n",
    "    f = open(random_sampling_file, \"r\")\n",
    "    for line in f:\n",
    "        brir, file = line.split(\" \")\n",
    "        initial_states.append((brir, file.removesuffix(\"\\n\")))\n",
    "    f.close()\n",
    "    return initial_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48c243",
   "metadata": {
    "id": "6b48c243"
   },
   "source": [
    "## Deep Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d12bec",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1718035494519,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "f9d12bec"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_angles):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_hidden = 1000\n",
    "        self.gru1 = nn.GRU(n_observations, 256, 1, batch_first=True, bidirectional=False)\n",
    "        self.gru2 = nn.GRU(256, 128, 1, batch_first=True, bidirectional=False)\n",
    "        self.gru3 = nn.GRU(128, 64, 1, batch_first=True, bidirectional=False)\n",
    "        self.fc = nn.Linear(64*2, n_angles)\n",
    "        self.dropout20 = nn.Dropout(p=0.2)\n",
    "        self.dropout50 = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru1(x)\n",
    "        x = self.dropout20(x)\n",
    "        x, _ = self.gru2(x)\n",
    "        x = self.dropout20(x)\n",
    "        x, _ = self.gru3(x)\n",
    "        x = self.dropout50(x)\n",
    "        x = torch.cat((x[:,0,:],x[:,1,:]), dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2211ee2f",
   "metadata": {
    "id": "2211ee2f"
   },
   "source": [
    "## Memory buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0a5f9c",
   "metadata": {
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1718035494880,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "ab0a5f9c"
   },
   "outputs": [],
   "source": [
    "class MemoryBuffer():\n",
    "    def __init__(self, buffer_length, batch_size):\n",
    "        self.buffer_length = buffer_length\n",
    "        self.idx = 0\n",
    "        self.s_buffer = torch.zeros((self.buffer_length, 2, 4000))\n",
    "        self.a_buffer = torch.zeros((self.buffer_length, 1), dtype=torch.int)\n",
    "        self.s1_buffer = torch.zeros((self.buffer_length, 2, 4000))\n",
    "        self.r_buffer = torch.zeros((self.buffer_length, 1))\n",
    "        self.t_buffer = torch.zeros((self.buffer_length, 1), dtype=torch.bool)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add_into_buffer(self, s, a, s_prime, r, t):\n",
    "        \"\"\"\n",
    "        Function that adds a <s,a,s',r> tuple into the buffer.\n",
    "        s: state\n",
    "        a: action\n",
    "        s_prime: new state\n",
    "        r: reward\n",
    "        \"\"\"\n",
    "        self.s_buffer[self.idx % self.buffer_length] = s\n",
    "        self.a_buffer[self.idx % self.buffer_length] = torch.tensor([a], dtype=torch.int)\n",
    "        self.s1_buffer[self.idx % self.buffer_length] = s_prime\n",
    "        self.r_buffer[self.idx % self.buffer_length] = torch.tensor([r])\n",
    "        self.t_buffer[self.idx % self.buffer_length] = torch.tensor([t], dtype=torch.bool)\n",
    "        if self.idx < self.buffer_length:\n",
    "            self.idx += 1\n",
    "        else:\n",
    "            self.idx += 2\n",
    "\n",
    "\n",
    "    def sample_from_buffer(self):\n",
    "        \"\"\"\n",
    "        Function that returns random samples from the buffer.\n",
    "        \"\"\"\n",
    "        indices = torch.randperm(min(self.idx, self.batch_size))[:self.batch_size]\n",
    "        return self.s_buffer[indices], self.a_buffer[indices], \\\n",
    "        self.s1_buffer[indices], self.r_buffer[indices], self.t_buffer[indices]\n",
    "\n",
    "\n",
    "class BalancedMemoryBuffer():\n",
    "    def __init__(self, batch_size, num_classes, az_angles, el_angles):\n",
    "        self.device = torch.device(\"cuda\")\n",
    "\n",
    "        self.az_angles = az_angles\n",
    "        self.el_angles = el_angles\n",
    "\n",
    "        self.buffer_length = 100000\n",
    "        self.start_buffer_length = 5000\n",
    "        self.class_buffer_length = 1000\n",
    "        self.num_classes = 65\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.start_s = torch.empty((self.start_buffer_length, 2, 4000), dtype=torch.float).to(self.device)\n",
    "        self.start_a = torch.empty((self.start_buffer_length, 1), dtype=torch.int).to(self.device)\n",
    "        self.start_s1 = torch.empty((self.start_buffer_length, 2, 4000), dtype=torch.float).to(self.device)\n",
    "        self.start_r = torch.empty((self.start_buffer_length, 1), dtype=torch.float).to(self.device)\n",
    "        self.start_t = torch.empty((self.start_buffer_length, 1), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        self.class_s = torch.empty((num_classes, self.class_buffer_length, 2, 4000), dtype=torch.float).to(self.device)\n",
    "        self.class_a = torch.empty((num_classes, self.class_buffer_length, 1), dtype=torch.int).to(self.device)\n",
    "        self.class_s1 = torch.empty((num_classes, self.class_buffer_length, 2, 4000), dtype=torch.float).to(self.device)\n",
    "        self.class_r = torch.empty((num_classes, self.class_buffer_length, 1), dtype=torch.float).to(self.device)\n",
    "        self.class_t = torch.empty((num_classes, self.class_buffer_length, 1), dtype=torch.bool).to(self.device)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.class_idxs = torch.zeros(num_classes, dtype=torch.int, device=self.device)\n",
    "\n",
    "\n",
    "    def find_idx(self, az, el):\n",
    "        az_angle_idx = self.az_angles.index(az)\n",
    "        el_angle_idx = self.el_angles.index(el)\n",
    "\n",
    "        return az_angle_idx + el_angle_idx*len(self.az_angles)\n",
    "\n",
    "    def add_into_buffer(self, s, a, s_prime, r, t, az, el):\n",
    "        \"\"\"\n",
    "        Function that adds a <s,a,s',r> tuple into the buffer for a specific class.\n",
    "        s: state\n",
    "        a: action\n",
    "        s_prime: new state\n",
    "        r: reward\n",
    "        t: terminal state\n",
    "        az: azimuth angle\n",
    "        el: elevation angle\n",
    "        \"\"\"\n",
    "        if self.idx < self.start_buffer_length:\n",
    "            self.start_s[self.idx] = s\n",
    "            self.start_a[self.idx] = torch.tensor([a], dtype=torch.int)\n",
    "            self.start_s1[self.idx] = s_prime\n",
    "            self.start_r[self.idx] = torch.tensor([r])\n",
    "            self.start_t[self.idx] = torch.tensor([t], dtype=torch.bool)\n",
    "\n",
    "            self.idx += 1\n",
    "        else:\n",
    "            class_idx = self.find_idx(az, el)\n",
    "            idx_for_class = self.class_idxs[class_idx]\n",
    "\n",
    "            self.class_s[class_idx, int(idx_for_class)%self.class_buffer_length] = s\n",
    "            self.class_a[class_idx, int(idx_for_class)%self.class_buffer_length] = torch.tensor([a], dtype=torch.int)\n",
    "            self.class_s1[class_idx, int(idx_for_class)%self.class_buffer_length] = s_prime\n",
    "            self.class_r[class_idx, int(idx_for_class)%self.class_buffer_length] = torch.tensor([r])\n",
    "            self.class_t[class_idx, int(idx_for_class)%self.class_buffer_length] = torch.tensor([t], dtype=torch.bool)\n",
    "\n",
    "            if idx_for_class % 20 == 0:\n",
    "                indices = torch.cat(\n",
    "                    (torch.randperm(\n",
    "                        min(\n",
    "                            idx_for_class.item(), self.class_buffer_length)\n",
    "                        ),\n",
    "                    torch.arange(\n",
    "                        start=min(\n",
    "                            idx_for_class.item(), self.class_buffer_length), end=self.class_buffer_length)\n",
    "                    )).to(self.device)\n",
    "\n",
    "                self.class_s[class_idx] = self.class_s[class_idx, indices, :, :]\n",
    "                self.class_a[class_idx] = self.class_a[class_idx, indices, :]\n",
    "                self.class_s1[class_idx] = self.class_s1[class_idx, indices, :, :]\n",
    "                self.class_r[class_idx] = self.class_r[class_idx, indices, :]\n",
    "                self.class_t[class_idx] = self.class_t[class_idx, indices, :]\n",
    "\n",
    "            self.class_idxs[class_idx] += 1\n",
    "\n",
    "    def sample_from_buffer(self):\n",
    "        \"\"\"\n",
    "        Function that returns random samples from the buffer with equal representation from each class.\n",
    "        \"\"\"\n",
    "        if self.idx < self.start_buffer_length:\n",
    "            indices = torch.randperm(min(self.idx, self.start_buffer_length))[:self.batch_size]\n",
    "            return self.start_s[indices], self.start_a[indices], \\\n",
    "            self.start_s1[indices], self.start_r[indices], self.start_t[indices]\n",
    "        else:\n",
    "            min_class_idx = torch.min(self.class_idxs)\n",
    "            cut_s = self.class_s[:,:min_class_idx,:,:]\n",
    "            cut_a = self.class_a[:,:min_class_idx,:]\n",
    "            cut_s1 = self.class_s1[:,:min_class_idx,:,:]\n",
    "            cut_r = self.class_r[:,:min_class_idx,:]\n",
    "            cut_t = self.class_t[:,:min_class_idx,:]\n",
    "\n",
    "            flat_s = cut_s.reshape(-1, cut_s.size(2), cut_s.size(3))\n",
    "            flat_a = cut_a.reshape(-1, cut_a.size(2))\n",
    "            flat_s1 = cut_s1.reshape(-1, cut_s1.size(2), cut_s1.size(3))\n",
    "            flat_r = cut_r.reshape(-1, cut_r.size(2))\n",
    "            flat_t = cut_t.reshape(-1, cut_t.size(2))\n",
    "\n",
    "            total_s = torch.cat((self.start_s, flat_s), axis=0)\n",
    "            total_a = torch.cat((self.start_a, flat_a), axis=0)\n",
    "            total_s1 = torch.cat((self.start_s1, flat_s1), axis=0)\n",
    "            total_r = torch.cat((self.start_r, flat_r), axis=0)\n",
    "            total_t = torch.cat((self.start_t, flat_t), axis=0)\n",
    "\n",
    "            # print(total_a)\n",
    "            max_index = total_s.size(0)\n",
    "\n",
    "            indices = torch.randperm(max_index)[:self.batch_size]#.to(self.device)\n",
    "            return total_s[indices], total_a[indices], total_s1[indices], total_r[indices], total_t[indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e1792",
   "metadata": {
    "id": "476e1792"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05082f3",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1718035494881,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "b05082f3"
   },
   "outputs": [],
   "source": [
    "class environment():\n",
    "\n",
    "    def __init__(self, fs, n_windows, length_windows, az_angles, el_angles, random_sampling_file, target_network, policy_network, memory_buffer, n_actions, actions, device):\n",
    "        self.device=device\n",
    "        self.fs = fs\n",
    "        self.n_windows = n_windows\n",
    "        self.length_windows = length_windows\n",
    "        self.az_angles = az_angles\n",
    "        self.el_angles = el_angles\n",
    "        self.init_states = initial_states_to_list(random_sampling_file)\n",
    "        self.target_network = target_network.to(device)\n",
    "        self.policy_network = policy_network.to(device)\n",
    "        self.memory_buffer = memory_buffer\n",
    "        self.actions = actions\n",
    "        self.n_actions = n_actions\n",
    "        self.batch_size = 1024\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "        self.lr = 0.00025\n",
    "        self.optimizer = torch.optim.AdamW(self.policy_network.parameters(), lr=self.lr, amsgrad=True)\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def get_initial_state(self, idx):\n",
    "        \"\"\"\n",
    "        Gives a new sample and initial BRIR/state for a new epoch.\n",
    "        idx: the index of the new epoch.\n",
    "        Returns: a tuple with the new BRIR and new sample.\n",
    "        \"\"\"\n",
    "        return self.init_states[idx]\n",
    "\n",
    "    def open_sample_split(self, sample_name):\n",
    "        \"\"\"\n",
    "        Gets the data of the sample, and splits it into the amount of specified windows.\n",
    "        sample_name: the name of the sample.\n",
    "        Returns: a 2D array with the sample split into windows.\n",
    "        \"\"\"\n",
    "        sample, _ = torchaudio.load(\"samples_10s/\"+sample_name, format=\"flac\")\n",
    "        sample = sample[:,::2]\n",
    "        return sample.reshape((self.n_windows,-1))\n",
    "\n",
    "    def get_azel_from_brir(self, brir_name):\n",
    "        return brir_name.split(\"_\")[16], brir_name.split(\"_\")[18][:3]\n",
    "\n",
    "    def open_brir(self, brir_name):\n",
    "        \"\"\"\n",
    "        Opens the BRIR given the name of the BRIR.\n",
    "        Downsamples the BRIR, as BRIR has Fs=16000Hz and the samples Fs=8000Hz\n",
    "        brir_name: the name of the BRIR.\n",
    "        Returns: the two channels (for each ear) of the BRIR.\n",
    "        \"\"\"\n",
    "        brir, brir_fs = torchaudio.load(\"BRIRs_downsampled/\"+brir_name, format=\"wav\")\n",
    "        return brir[:,::2]\n",
    "\n",
    "    def convolve_sound(self, window, brir):\n",
    "        \"\"\"\n",
    "        Convolves the BRIR with the window, and cuts result at the size of the window length.\n",
    "        window: the window of the sample that should be convolved.\n",
    "\n",
    "        \"\"\"\n",
    "        return torchaudio.functional.convolve(window.repeat([2,1]), brir)[:,:self.length_windows]\n",
    "        # return torchaudio.functional.convolve(windows[0].reshape(1,-1).repeat([2,1]), brir)[:,:self.length_windows]\n",
    "\n",
    "    def get_Q_values_from_state(self, observation):\n",
    "        \"\"\"\n",
    "        Puts observation into target network and chooses actions\n",
    "        observation: the sample convolved with the HRTF\n",
    "        \"\"\"\n",
    "        return self.target_network(observation)\n",
    "\n",
    "    def get_best_actions_from_Q_values(self, Q_values):\n",
    "        \"\"\"\n",
    "        Chooses the best action index given the Q-values for the azimuth and elevation angle rotation.\n",
    "        Q_values: the Q-values as returned by the target network.\n",
    "        Returns: index of the best action\n",
    "        \"\"\"\n",
    "        return torch.argmax(Q_values)\n",
    "        # return torch.min(torch.floor(torch.add(Q_values, 1)*(self.n_directions/2)),torch.tensor(self.n_directions-1))\n",
    "\n",
    "    def sample_action_epsilon_greedily(self, best_action, epsilon):\n",
    "        \"\"\"\n",
    "        Samples an action epsilon-greedily.\n",
    "        best_actions: the index of the best actions.\n",
    "        epsilon: the greediness parameter.\n",
    "        Returns: the actions [az,el].\n",
    "        \"\"\"\n",
    "        p_best_action = 1 - epsilon + epsilon/self.n_actions\n",
    "        p_action = epsilon/self.n_actions\n",
    "        probability_table = np.full(self.n_actions, p_action)\n",
    "        probability_table[best_action] = p_best_action\n",
    "        index = np.random.choice(np.arange(self.n_actions), p=probability_table)\n",
    "        return self.actions[index], index\n",
    "\n",
    "    def take_action(self, actions, az, el):\n",
    "        \"\"\"\n",
    "        Finds the next azimuth and elevation angle after taking the action.\n",
    "        actions: a tensor with two values for the azimuth and elevation angles.\n",
    "        current_brir: the current filename.\n",
    "        Returns: the new azimuth angle and the new elevation angle, after taking the action, as well as the old angles.\n",
    "        \"\"\"\n",
    "\n",
    "        current_az_angle_idx = self.az_angles.index(az)\n",
    "        current_el_angle_idx = self.el_angles.index(el)\n",
    "\n",
    "        az_mov, el_mov = actions\n",
    "\n",
    "        if current_az_angle_idx + az_mov >= len(self.az_angles):\n",
    "            az_mov = 0\n",
    "        if current_az_angle_idx + az_mov < 0:\n",
    "            az_mov = 0\n",
    "        if current_el_angle_idx + el_mov >= len(self.el_angles):\n",
    "            el_mov = 0\n",
    "        if current_el_angle_idx + el_mov < 0:\n",
    "            el_mov = 0\n",
    "\n",
    "        return self.az_angles[current_az_angle_idx + az_mov], self.el_angles[current_el_angle_idx + el_mov]\n",
    "\n",
    "    def get_reward(self, new_az, new_el, old_az, old_el, brir_name):\n",
    "        \"\"\"\n",
    "        Gives reward based on Euclidean distance to the goal.\n",
    "        new_az: the new azimuth angle\n",
    "        new_el: the new elevation angle\n",
    "        old_az: the old azimuth angle\n",
    "        old_el: the old elevation angle\n",
    "        current_brir: the name of the BRIR\n",
    "        \"\"\"\n",
    "        split_brir = brir_name.split(\"_\")\n",
    "        goal_az = split_brir[12]\n",
    "        goal_el = split_brir[14]\n",
    "\n",
    "        if new_az == goal_az and new_el == goal_el:\n",
    "            return 1\n",
    "\n",
    "        dists = np.zeros(n_actions)\n",
    "        for i, action in enumerate(self.actions):\n",
    "            temp_az, temp_el = self.take_action(action, old_az, old_el)\n",
    "            dists[i] = np.sqrt((self.az_angles.index(temp_az) - self.az_angles.index(goal_az))**2 + (self.el_angles.index(temp_el) - self.el_angles.index(goal_el))**2)\n",
    "        optimal_dist = np.min(dists)\n",
    "        actual_dist = np.sqrt((self.az_angles.index(new_az) - self.az_angles.index(goal_az))**2 + (self.el_angles.index(new_el) - self.el_angles.index(goal_el))**2)\n",
    "        old_dist = np.sqrt((self.az_angles.index(old_az) - self.az_angles.index(goal_az))**2 + (self.el_angles.index(old_el) - self.el_angles.index(goal_el))**2)\n",
    "\n",
    "        if optimal_dist == actual_dist:\n",
    "            return 0.1    # Optimal distance improvement\n",
    "        elif old_dist > actual_dist:\n",
    "            return 0      # Suboptimal distance improvement\n",
    "        else:\n",
    "            return -0.2   # Distance detoriation\n",
    "\n",
    "    def next_state(self, current_brir, new_az, new_el):\n",
    "        \"\"\"\n",
    "        Finds the name of the next BRIR.\n",
    "        current_brir: the name of the current brir\n",
    "        new_az: the new azimuth angle after the action.\n",
    "        el_mov: the new elevation angle after the action.\n",
    "        Returns: the filename of the BRIR.\n",
    "        \"\"\"\n",
    "\n",
    "        brir_name = current_brir.split(\"_\")\n",
    "        brir_name[16] = new_az\n",
    "        brir_name[18] = new_el+\".wav\"\n",
    "\n",
    "        return \"_\".join(brir_name)\n",
    "\n",
    "    def is_terminal(self, current_brir, az, el):\n",
    "        \"\"\"\n",
    "        Returns whether the agent is on a terminal state (when the speaker and agent have the smae angle)\n",
    "        \"\"\"\n",
    "        brir_name = current_brir.split(\"_\")\n",
    "        return brir_name[12] == az and brir_name[14] == el\n",
    "\n",
    "    def append_buffer(self, observation, action_indices, new_observation, reward, terminal, az, el):\n",
    "        \"\"\"\n",
    "        Appends an observation into the buffer.\n",
    "        observation: state s_t\n",
    "        action_indices: action a_t\n",
    "        new_observation: new state s_(t+1)\n",
    "        reward: r\n",
    "        terminal: boolean t\n",
    "        az: azimuth angle of new state\n",
    "        el: elevation angle of new state\n",
    "        \"\"\"\n",
    "        self.memory_buffer.add_into_buffer(observation, action_indices, new_observation, reward, terminal, az, el)\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the policy network.\n",
    "        returns: the loss as computed with the Huber loss function.\n",
    "        \"\"\"\n",
    "        if self.memory_buffer.idx < 1+self.batch_size:\n",
    "            return\n",
    "\n",
    "        s_batch, a_batch, s1_batch, r_batch, t_batch = self.memory_buffer.sample_from_buffer()\n",
    "        a_batch = a_batch.type(torch.int64)\n",
    "\n",
    "        s_batch = s_batch.to(device)\n",
    "        a_batch = a_batch.to(device)\n",
    "        s1_batch = s1_batch.to(device)\n",
    "        r_batch = r_batch.to(device)\n",
    "        t_batch = t_batch.to(device)\n",
    "\n",
    "        Q_vals = self.policy_network(s_batch).to(device)\n",
    "\n",
    "        Q_vals = Q_vals.gather(1, a_batch).squeeze(1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Q1_vals = self.target_network(s1_batch).max(1)[0].detach().to(device)\n",
    "\n",
    "        target_Q_vals = r_batch.squeeze(1) + self.gamma * Q1_vals * (~t_batch.squeeze(1))\n",
    "\n",
    "        loss = nn.functional.mse_loss(Q_vals, target_Q_vals)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def distance(self, az, el, brir_name):\n",
    "        \"\"\"\n",
    "        Gives the least amount of actions needed to get to the goal (Chebishev distance)\n",
    "        az: current azimuth angle\n",
    "        el: current elevation angle\n",
    "        brir_name: name of the BRIR file\n",
    "        returns: the least amount of actions needed to get to the goal\n",
    "        \"\"\"\n",
    "        split_brir = brir_name.split(\"_\")\n",
    "        goal_az = split_brir[12]\n",
    "        goal_el = split_brir[14]\n",
    "\n",
    "        return max(abs(self.az_angles.index(az) - self.az_angles.index(goal_az)), abs(self.el_angles.index(el) - self.el_angles.index(goal_el)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430129c0",
   "metadata": {
    "executionInfo": {
     "elapsed": 4843,
     "status": "ok",
     "timestamp": 1718035499721,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "430129c0"
   },
   "outputs": [],
   "source": [
    "n_observations = 4000\n",
    "n_actions = 8\n",
    "actions = torch.tensor([[-1,-1],[-1,0],[-1,1],[0,-1],[0,1],[1,-1],[1,0],[1,1]])\n",
    "\n",
    "policy_network = DQN(n_observations, n_actions).to(device)\n",
    "target_network = DQN(n_observations, n_actions).to(device)\n",
    "\n",
    "# As weights of network get randomly initialized, copy all weights from target network into policy network\n",
    "policy_network.load_state_dict(target_network.state_dict())\n",
    "\n",
    "az_angles = [\"270\", \"285\", \"300\", \"315\", \"330\", \"345\", \"000\", \"015\", \"030\", \"045\", \"060\", \"075\", \"090\"]\n",
    "el_angles = [\"-45\", \"-20\", \"000\", \"020\", \"045\"]\n",
    "\n",
    "memory_buffer = BalancedMemoryBuffer(1024, 65, az_angles, el_angles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iN5jhGHXzTmc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15572,
     "status": "ok",
     "timestamp": 1718035515283,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "iN5jhGHXzTmc",
    "outputId": "4130ef59-95f5-48e4-82fd-c50802ae4536"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VRMPD3ECzHo2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43435,
     "status": "ok",
     "timestamp": 1718035559480,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "VRMPD3ECzHo2",
    "outputId": "3169f885-273f-459d-abef-476fca472067"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/My Drive/samples_10s.zip\" -d \"/content\"\n",
    "!unzip \"/content/drive/My Drive/BRIRs_16000Hz.zip\" -d \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f2b8d",
   "metadata": {
    "executionInfo": {
     "elapsed": 3782,
     "status": "ok",
     "timestamp": 1718035566752,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "cc9f2b8d"
   },
   "outputs": [],
   "source": [
    "# Making the environment\n",
    "fs = 8000\n",
    "n_windows = 20\n",
    "length_windows = 4000\n",
    "az_angles = [\"270\", \"285\", \"300\", \"315\", \"330\", \"345\", \"000\", \"015\", \"030\", \"045\", \"060\", \"075\", \"090\"]\n",
    "el_angles = [\"-45\", \"-20\", \"000\", \"020\", \"045\"]\n",
    "random_sampling_file = \"/content/drive/My Drive/random_sampling_file_brir.txt\"\n",
    "\n",
    "env = environment(fs, n_windows, length_windows, az_angles, el_angles, random_sampling_file, target_network, policy_network, memory_buffer, n_actions, actions, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89456f42",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7590952,
     "status": "ok",
     "timestamp": 1718044421796,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "89456f42",
    "outputId": "cf7cb9f0-4440-4728-e77d-890e70cf3103"
   },
   "outputs": [],
   "source": [
    "episodes = 62400\n",
    "epsilon = 0.2\n",
    "episode_length = list() #\n",
    "all_loss = list() #\n",
    "tau = 0.005\n",
    "start_distances = list() #\n",
    "end_distances = list() #\n",
    "absolute_distances = list() #\n",
    "relative_distances = list() #\n",
    "trajectories = list() #\n",
    "all_rewards = list()\n",
    "goals = list()\n",
    "\n",
    "for ep in tqdm(range(episodes)):\n",
    "    trajectory = list()\n",
    "    total_reward = 0\n",
    "\n",
    "    # Initialise s1:\n",
    "    brir_name, samp_name = env.get_initial_state(ep)\n",
    "    goals.append((brir_name.split(\"_\")[12], brir_name.split(\"_\")[14]))\n",
    "    az, el = env.get_azel_from_brir(brir_name)\n",
    "    start_distances.append(env.distance(az, el, brir_name))\n",
    "    trajectory.append([int(az) if int(az) <= 90 else -((-int(az))%360), int(el)])\n",
    "    windows = env.open_sample_split(samp_name).to(device)\n",
    "    brir = torch.tensor(env.open_brir(brir_name), dtype=torch.float, device=device)\n",
    "    observation = env.convolve_sound(windows[0:1], brir)[0:2].to(device)\n",
    "\n",
    "    # Loop over all other windows:\n",
    "    for window in range(1, env.n_windows):\n",
    "        # Select action epsilon-greedily:\n",
    "        Q_vals = env.get_Q_values_from_state(observation.unsqueeze(0))\n",
    "        best_action = env.get_best_actions_from_Q_values(Q_vals)\n",
    "        actions, action_idx = env.sample_action_epsilon_greedily(best_action, epsilon)\n",
    "\n",
    "        # Execute action and observe reward:\n",
    "        new_az, new_el = env.take_action(actions, az, el)\n",
    "        trajectory.append([int(new_az) if int(new_az) <= 90 else -((-int(new_az))%360), int(new_el)])\n",
    "        reward = env.get_reward(new_az, new_el, az, el, brir_name)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Set new action:\n",
    "        terminal = env.is_terminal(brir_name, new_az, new_el)\n",
    "        if not terminal:\n",
    "            brir_name = env.next_state(brir_name, new_az, new_el)\n",
    "            # print(brir_name)\n",
    "            brir = torch.tensor(env.open_brir(brir_name), dtype=torch.float, device=device)\n",
    "            new_observation = env.convolve_sound(windows[window:window+1], brir)[0:2].to(device)\n",
    "        else:\n",
    "            new_observation = torch.zeros(2, length_windows).to(device)\n",
    "\n",
    "\n",
    "        # Store into buffer:\n",
    "        env.append_buffer(observation, action_idx, new_observation, reward, terminal, new_az, new_el)\n",
    "\n",
    "        # Go to next state:\n",
    "        az, el = new_az, new_el\n",
    "        observation = new_observation\n",
    "\n",
    "        if terminal:\n",
    "            episode_length.append(window)\n",
    "            # DO STUFF FOR STATISTICS\n",
    "            break\n",
    "        if window == 19:\n",
    "            episode_length.append(window+1)\n",
    "\n",
    "    end_distances.append(env.distance(az, el, brir_name))\n",
    "    all_rewards.append(total_reward)\n",
    "\n",
    "    absolute_distances.append(start_distances[-1] - end_distances[-1])\n",
    "    if end_distances[-1] != 0:\n",
    "        relative_distances.append(start_distances[-1] / end_distances[-1])\n",
    "    else:\n",
    "        relative_distances.append(10)\n",
    "\n",
    "    # Optimize model\n",
    "    loss = env.train_model()\n",
    "    all_loss.append(loss)\n",
    "\n",
    "    trajectories.append(trajectory)\n",
    "\n",
    "    if ep%240 == 0:\n",
    "        # Update model (hard or soft?)\n",
    "        # Hard update:\n",
    "        # target_net = env.target_network.state_dict()\n",
    "        # policy_net = env.policy_network.state_dict()\n",
    "        # for key in policy_net:\n",
    "        #     target_net[key] = policy_net[key]*tau + target_net[key] * (1-tau)\n",
    "        # env.target_network.load_state_dict(target_net)\n",
    "        env.target_network.load_state_dict(env.policy_network.state_dict())\n",
    "        print(np.mean(episode_length[-240:]), all_loss[-1], np.mean(all_rewards[-240:]))\n",
    "        torch.save(env.policy_network.state_dict(), 'policy_network_architecture_hrtf_4.pth')\n",
    "\n",
    "    # if ep%2400 == 0:\n",
    "    #     plt.figure()\n",
    "    #     plt.plot(np.array(trajectory).T[0], np.array(trajectory).T[1])\n",
    "    #     plt.xlim([-95,95])\n",
    "    #     plt.ylim([-50,50])\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    epsilon -= (epsilon/episodes)\n",
    "\n",
    "print(np.mean(episode_length[-240:]), all_loss[-1])\n",
    "torch.save(env.policy_network.state_dict(), 'hrtf_weights.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dffa5f8-f986-402a-9140-d72b09ed19a3",
   "metadata": {},
   "source": [
    "## Make plots:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625f6efb-7241-4d33-b45d-edb4207834ee",
   "metadata": {},
   "source": [
    "### Draw trajectories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DoWDM5wbvKlW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1jm_5CsorRMX3XJHcGF5YREEX62_UrU_t"
    },
    "executionInfo": {
     "elapsed": 24380,
     "status": "ok",
     "timestamp": 1718045045950,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "DoWDM5wbvKlW",
    "outputId": "fec9be34-5cda-48be-f63c-896226adc9eb"
   },
   "outputs": [],
   "source": [
    "colors = ['red', 'orange', 'yellow', 'green', 'lime', 'cyan', 'magenta', 'pink', 'purple', 'brown']\n",
    "for i in range(0,62400, 2000):\n",
    "    plt.figure()\n",
    "    for j in range(10):\n",
    "        plt.plot(np.array(trajectories[i+j]).T[0], np.array(trajectories[i+j]).T[1], label=\"trajectory \"+str(j+1)+\", episode length \"+str(episode_length[i+j])+ \", reward \"+str(all_rewards[i+j]).format(\"%2d\"), color=colors[j])\n",
    "        plt.scatter(np.array(trajectories[i+j]).T[0][0], np.array(trajectories[i+j]).T[1][0], marker='o', color=colors[j])\n",
    "        plt.scatter(np.array(trajectories[i+j]).T[0][-1], np.array(trajectories[i+j]).T[1][-1], marker='x', color=colors[j])\n",
    "        # print(episode_length[i+j])\n",
    "        plt.scatter(np.array(int(goals[i+j][0])), np.array(int(goals[i+j][1])), marker='*', color=colors[j])\n",
    "    plt.xlim([-95,95])\n",
    "    plt.ylim([-50,50])\n",
    "    plt.xticks([90, 75, 60, 45, 30, 15, 0, -15, -30, -45, -60, -75, -90])\n",
    "    plt.yticks([45, 20, 0, -20, -45])\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(\"Trajectory at episodes \"+ str(i)+\" to \"+ str(i+9))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dcf185-c025-4fd0-93f7-44fac10b4360",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1082,
     "status": "ok",
     "timestamp": 1718044472323,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "pDojrTw4wR3x",
    "outputId": "06221283-c2c2-49e8-de30-714b6f27497b"
   },
   "source": [
    "### Plot end distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hDOWoiwCwhpx",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1703,
     "status": "ok",
     "timestamp": 1718044476227,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "hDOWoiwCwhpx",
    "outputId": "26294c39-0595-4ddd-966f-183ccc505483"
   },
   "outputs": [],
   "source": [
    "ma_abs = list()\n",
    "for i in range(62200):\n",
    "    ma_abs.append(np.mean(np.array(end_distances[i:i+200])))\n",
    "plt.figure()\n",
    "plt.plot(ma_abs)\n",
    "plt.title(\"Moving average of distance to goal at end of episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Chebishev distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25fdbd7-9262-4aea-a5f3-82c173cef587",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1101,
     "status": "ok",
     "timestamp": 1718044503472,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "liMqJZb4JsCk",
    "outputId": "26fc4db9-48b6-481e-a97c-0130920f5aa6"
   },
   "source": [
    "### Plot reward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8m92xqNcJylB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 1476,
     "status": "ok",
     "timestamp": 1718044507509,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "8m92xqNcJylB",
    "outputId": "9edc24f2-0015-40ac-ca76-9073412b8748"
   },
   "outputs": [],
   "source": [
    "ma_rew = list()\n",
    "for i in range(62200):\n",
    "    ma_rew.append(np.mean(np.array(all_rewards[i:i+200])))\n",
    "plt.figure()\n",
    "plt.plot(ma_rew)\n",
    "plt.title(\"Moving average of reward\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8085bc-7626-4086-9de0-36fb859775e9",
   "metadata": {},
   "source": [
    "### Plot episode length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef1276d-7461-4905-9fbe-577f4b6ec70b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1718044609804,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "bef1276d-7461-4905-9fbe-577f4b6ec70b",
    "outputId": "324218ce-217f-40d1-d32e-435c85987a5a"
   },
   "outputs": [],
   "source": [
    "ma = list()\n",
    "for i in range(62200):\n",
    "    ma.append(np.mean(np.array(episode_length[i:i+200])))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ma)\n",
    "plt.title(\"Moving average of episode length\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Episode length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d475deb-5e77-4f87-ad0c-18227c91298d",
   "metadata": {},
   "source": [
    "### Plot loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcec22-505a-4b94-8cb2-3684b0c83270",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "executionInfo": {
     "elapsed": 701,
     "status": "ok",
     "timestamp": 1718044621882,
     "user": {
      "displayName": "Wessel Ledder",
      "userId": "03089760460695138149"
     },
     "user_tz": -120
    },
    "id": "64fcec22-505a-4b94-8cb2-3684b0c83270",
    "outputId": "e0b9afb3-9fdf-437b-c538-cd17aca652e1"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_loss)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Loss (Huber)\")\n",
    "plt.title(\"Loss during training\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
